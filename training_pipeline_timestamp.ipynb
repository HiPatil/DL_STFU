{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd423b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchaudio.transforms import MelSpectrogram, ComputeDeltas\n",
    "\n",
    "from torch.optim.adamw import AdamW\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import tqdm\n",
    "import IPython\n",
    "import re\n",
    "\n",
    "import whisper_timestamped as whisper\n",
    "import librosa\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import ast\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c093b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wake_words = [\n",
    "    \"fuck\"\n",
    "    ,\"fucking\"\n",
    "    ,\"ass\"\n",
    "    ,\"bitch\"\n",
    "    ,\"dick\"\n",
    "    ,\"fag\"\n",
    "    ,\"faggot\"\n",
    "    ,\"shit\"\n",
    "    ,\"cunt\"\n",
    "    ,\"whore\"\n",
    "    ,\"cock\"\n",
    "    ,\"pussy\"\n",
    "    ,\"cocksucker\"\n",
    "\n",
    "]\n",
    "\n",
    "wake_words_sequence = [\"0\", \"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f85c694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wake_word_seq_map = dict(zip(wake_words, wake_words_sequence))\n",
    "regex_pattern = r\"\\b(?:{})\\b\".format(\"|\".join(map(re.escape, wake_words)))\n",
    "pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def wake_words_search(pattern, word):\n",
    "    try:\n",
    "        return bool(pattern.search(word))\n",
    "    except TypeError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5827fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_data = pd.read_csv('cuss_word_time_stamp_wisper_true.csv')\n",
    "positive_train_data['timestamps'] = positive_train_data['timestamps'].apply(ast.literal_eval)\n",
    "positive_train_data['timestamps'] = positive_train_data['timestamps'].apply(lambda x: {key.replace(',', '').replace('.', ''): value for key, value in x.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9391ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109fd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_pattern = re.compile(r\"'(?P<key>[^']+)'\")\n",
    "\n",
    "def compute_labels(metadata, audio_data):\n",
    "  label = len(wake_words) # by default negative label\n",
    "\n",
    "  # if it is generated data then \n",
    "  if metadata['sentence'].lower() in wake_words:\n",
    "    label = int(wake_word_seq_map[metadata['sentence'].lower()])\n",
    "  else:\n",
    "    # if the sentence has one wakeword get label and end timestamp\n",
    "    for word in metadata['sentence'].lower().split():\n",
    "      wake_word_found = False\n",
    "      word = re.sub('\\W+', '', word)\n",
    "      if word in wake_words:\n",
    "        wake_word_found = True\n",
    "        break\n",
    "  \n",
    "    if wake_word_found:\n",
    "      label = int(wake_word_seq_map[word])\n",
    "      if word in  metadata['timestamps']:\n",
    "        timestamps = metadata['timestamps']\n",
    "        if type(timestamps) == str:\n",
    "          timestamps = json.loads(key_pattern.sub(r'\"\\g\"', timestamps))\n",
    "        word_ts = timestamps[word]\n",
    "        audio_start_idx = int((word_ts['start'] * 1000) * sr / 1000)\n",
    "        audio_end_idx = int((word_ts['end'] * 1000) * sr / 1000)\n",
    "        audio_data = audio_data[audio_start_idx:audio_end_idx]\n",
    "      else: # if there are issues with word alignment, we might not get ts\n",
    "        label = len(wake_words)  # mark them for negative\n",
    "\n",
    "  return label, audio_data\n",
    "\n",
    "\n",
    "\n",
    "class AudioCollator(object):\n",
    "  def __init__(self, noise_set=None):\n",
    "    self.noise_set = noise_set\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    batch_tensor = {}\n",
    "    window_size_ms = 750\n",
    "    max_length = int(window_size_ms/1000 * sr)\n",
    "    audio_tensors = []\n",
    "    labels = []\n",
    "    for sample in batch:\n",
    "      # get audio_data in tensor format\n",
    "      audio_data = librosa.core.load(data_path+sample['path'], sr=sr, mono=True)[0]\n",
    "      # get the label and its audio\n",
    "      label, audio_data = compute_labels(sample, audio_data)\n",
    "      audio_data_length = audio_data.size / sr * 1000 #ms\n",
    "  \n",
    "      # below is to make sure that we always got length of 12000 \n",
    "      # i.e 750 ms with sr 16000\n",
    "      # trim to max_length\n",
    "      if audio_data_length > window_size_ms:\n",
    "        # randomly trim either at start and end\n",
    "        if random.random() < 0.5:\n",
    "          audio_data = audio_data[:max_length]\n",
    "        else:\n",
    "          audio_data = audio_data[audio_data.size-max_length:]\n",
    "  \n",
    "      # pad with zeros\n",
    "      if audio_data_length < window_size_ms:\n",
    "        # randomly either append or prepend\n",
    "        if random.random() < 0.5:\n",
    "          audio_data = np.append(audio_data, np.zeros(int(max_length - audio_data.size)))\n",
    "        else:\n",
    "          audio_data = np.append(np.zeros(int(max_length - audio_data.size)), audio_data)\n",
    "\n",
    "      # Add noise\n",
    "      if self.noise_set:\n",
    "        noise_level =  random.randint(1, 5)/10 # 10 to 50%\n",
    "        noise_sample = librosa.core.load(self.noise_set[random.randint(0,len(self.noise_set)-1)], sr=sr, mono=True)[0]\n",
    "        # randomly select first or last seq of noise\n",
    "        if random.random() < 0.5:\n",
    "          audio_data = (1 - noise_level) * audio_data +  noise_level * noise_sample[:max_length]\n",
    "        else:\n",
    "          audio_data = (1 - noise_level) * audio_data +  noise_level * noise_sample[-max_length:]\n",
    "  \n",
    "      audio_tensors.append(torch.from_numpy(audio_data))\n",
    "      labels.append(label)\n",
    "  \n",
    "    batch_tensor = {\n",
    "        'audio': torch.stack(audio_tensors),\n",
    "        'labels': torch.tensor(labels) \n",
    "    }\n",
    "    \n",
    "    return batch_tensor\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efc9f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = positive_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc9d4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fuck word 19.54486416278818 %\n",
      "Total ass word 3.144279683948479 %\n",
      "Total fucking word 42.78872172313021 %\n"
     ]
    }
   ],
   "source": [
    "hey_pattern = re.compile(r'\\bfuck\\b', flags=re.IGNORECASE)\n",
    "fourth_pattern = re.compile(r'\\bass\\b', flags=re.IGNORECASE)\n",
    "brain_pattern = re.compile(r'\\bfucking\\b', flags=re.IGNORECASE)\n",
    "\n",
    "print(f\"Total fuck word {(train_ds[[wake_words_search(hey_pattern, sentence) for sentence in train_ds['sentence']]].size/train_ds.size) * 100} %\")\n",
    "print(f\"Total ass word {(train_ds[[wake_words_search(fourth_pattern, sentence) for sentence in train_ds['sentence']]].size/train_ds.size) * 100} %\")\n",
    "print(f\"Total fucking word {(train_ds[[wake_words_search(brain_pattern, sentence) for sentence in train_ds['sentence']]].size/train_ds.size) * 100} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a985ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_test = glob.glob('dataset/MS-SNSD/noise_test/*.wav')\n",
    "noise_train = glob.glob('dataset/MS-SNSD/noise_train/*.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9805489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 0\n",
    "\n",
    "train_audio_collator = AudioCollator(noise_set=noise_train)\n",
    "train_dl = tud.DataLoader(train_ds.to_dict(orient='records'),\n",
    "                  batch_size=batch_size,\n",
    "                  drop_last=True,\n",
    "                  shuffle=True,\n",
    "                  num_workers=num_workers,\n",
    "                  collate_fn=train_audio_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a433614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 24000\n",
    "\n",
    "num_mels = 40 # https://en.wikipedia.org/wiki/Mel_scale\n",
    "num_fft = 512 # window length - Fast Fourier Transform\n",
    "hop_length = 200  # making hops of size hop_length each time to sample the next window\n",
    "\n",
    "def audio_transform(audio_data):\n",
    "  # Transformations\n",
    "  # Mel-scale spectrogram is a combination of Spectrogram and mel scale conversion\n",
    "  # 1. compute FFT - for each window to transform from time domain to frequency domain\n",
    "  # 2. Generate Mel Scale - Take entire freq spectrum & seperate to n_mels evenly spaced\n",
    "  #    frequencies. (not by distance on freq domain but distance as it is heard by human ear)\n",
    "  # 3. Generate Spectrogram - For each window, decompose the magnitude of the signal\n",
    "  #    into its components, corresponding to the frequencies in the mel scale. \n",
    "  mel_spectrogram  = MelSpectrogram(n_mels=num_mels,\n",
    "                                    sample_rate=sr,\n",
    "                                    n_fft=num_fft,\n",
    "                                    hop_length=hop_length,\n",
    "                                    norm='slaney')\n",
    "  mel_spectrogram.to(device)\n",
    "  log_mels = mel_spectrogram(audio_data.float()).add_(1e-7).log_().contiguous()\n",
    "  # returns (channel, n_mels, time) \n",
    "  return log_mels.to(device)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f53c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "class ZmuvTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer('total', torch.zeros(1))\n",
    "        self.register_buffer('mean', torch.zeros(1))\n",
    "        self.register_buffer('mean2', torch.zeros(1))\n",
    "\n",
    "    def update(self, data, mask=None):\n",
    "        with torch.no_grad():\n",
    "            if mask is not None:\n",
    "                data = data * mask\n",
    "                mask_size = mask.sum().item()\n",
    "            else:\n",
    "                mask_size = data.numel()\n",
    "            self.mean = (data.sum() + self.mean * self.total) / (self.total + mask_size)\n",
    "            self.mean2 = ((data ** 2).sum() + self.mean2 * self.total) / (self.total + mask_size)\n",
    "            self.total += mask_size\n",
    "\n",
    "    def initialize(self, iterable: Iterable[torch.Tensor]):\n",
    "        for ex in iterable:\n",
    "            self.update(ex)\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return (self.mean2 - self.mean ** 2).sqrt()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / self.std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06866db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19fc3b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean is 0.000011\n",
      "Standard Deviation is 0.104003\n"
     ]
    }
   ],
   "source": [
    "zmuv_audio_collator = AudioCollator()\n",
    "zmuv_dl = tud.DataLoader(train_ds.to_dict(orient='records'),\n",
    "                  batch_size=1,\n",
    "                  num_workers=num_workers,\n",
    "                  collate_fn=zmuv_audio_collator)\n",
    "\n",
    "zmuv_transform = ZmuvTransform().to(device)\n",
    "if Path(\"zmuv.pt.bin\").exists():\n",
    "    zmuv_transform.load_state_dict(torch.load(str(\"zmuv.pt.bin\")))\n",
    "else:\n",
    "  for idx, batch in enumerate(zmuv_dl):\n",
    "    zmuv_transform.update(batch['audio'].to(device))\n",
    "  print(dict(zmuv_mean=zmuv_transform.mean, zmuv_std=zmuv_transform.std))\n",
    "  torch.save(zmuv_transform.state_dict(), str(\"zmuv.pt.bin\"))\n",
    "\n",
    "print(f\"Mean is {zmuv_transform.mean.item():0.6f}\")\n",
    "print(f\"Standard Deviation is {zmuv_transform.std.item():0.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fef5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_labels, num_maps1, num_maps2, num_hidden_input, hidden_size):\n",
    "        super(CNN, self).__init__()\n",
    "        conv0 = nn.Conv2d(1, num_maps1, (8, 16), padding=(4, 0), stride=(2, 2), bias=True)\n",
    "        pool = nn.MaxPool2d(2)\n",
    "        conv1 = nn.Conv2d(num_maps1, num_maps2, (5, 5), padding=2, stride=(2, 1), bias=True)\n",
    "        self.num_hidden_input = num_hidden_input\n",
    "        self.encoder1 = nn.Sequential(conv0,\n",
    "                                      nn.ReLU(),\n",
    "                                      pool,\n",
    "                                      nn.BatchNorm2d(num_maps1, affine=True))\n",
    "        self.encoder2 = nn.Sequential(conv1,\n",
    "                                      nn.ReLU(),\n",
    "                                      pool,\n",
    "                                      nn.BatchNorm2d(num_maps2, affine=True))\n",
    "        self.output = nn.Sequential(nn.Linear(num_hidden_input, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(0.1),\n",
    "                                    nn.Linear(hidden_size, num_labels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 1, 3, 2)  # change to (time, n_mels)\n",
    "        # pass through first conv layer\n",
    "        x1 = self.encoder1(x)\n",
    "        # pass through second conv layer\n",
    "        x2 = self.encoder2(x1)\n",
    "        # flattening - keep first dim batch same, flatten last 3 dims\n",
    "        x = x2.view(x2.size(0), -1)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b308c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(wake_words)+1 # oov\n",
    "num_maps1  = 48\n",
    "num_maps2  = 64\n",
    "num_hidden_input =  1152\n",
    "hidden_size = 128\n",
    "model = CNN(num_labels, num_maps1, num_maps2, num_hidden_input, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8909dbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (encoder1): Sequential(\n",
       "    (0): Conv2d(1, 48, kernel_size=(8, 16), stride=(2, 2), padding=(4, 0))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (encoder2): Sequential(\n",
       "    (0): Conv2d(48, 64, kernel_size=(5, 5), stride=(2, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (output): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78ac017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 48, 46, 13]           6,192\n",
      "              ReLU-2           [-1, 48, 46, 13]               0\n",
      "         MaxPool2d-3            [-1, 48, 23, 6]               0\n",
      "         MaxPool2d-4            [-1, 48, 23, 6]               0\n",
      "       BatchNorm2d-5            [-1, 48, 23, 6]              96\n",
      "            Conv2d-6            [-1, 64, 12, 6]          76,864\n",
      "              ReLU-7            [-1, 64, 12, 6]               0\n",
      "         MaxPool2d-8             [-1, 64, 6, 3]               0\n",
      "         MaxPool2d-9             [-1, 64, 6, 3]               0\n",
      "      BatchNorm2d-10             [-1, 64, 6, 3]             128\n",
      "           Linear-11                  [-1, 128]         147,584\n",
      "             ReLU-12                  [-1, 128]               0\n",
      "          Dropout-13                  [-1, 128]               0\n",
      "           Linear-14                   [-1, 14]           1,806\n",
      "================================================================\n",
      "Total params: 232,670\n",
      "Trainable params: 232,670\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.69\n",
      "Params size (MB): 0.89\n",
      "Estimated Total Size (MB): 1.59\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.to('cuda'), input_size=(1,40,91))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a2b065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001 # Weight regularization\n",
    "lr_decay = 0.95\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(filter(lambda x: x.requires_grad, model.parameters()))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a952c115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/20 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='1645' class='' max='2309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      71.24% [1645/2309 00:16&lt;00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:16, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#pbar = tqdm(train_dl, total=len(train_dl), position=0, desc=\"Training\", leave=True)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar(train_dl, parent\u001b[38;5;241m=\u001b[39mmb):\n\u001b[1;32m     21\u001b[0m   audio_data \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m   labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/fastprogress/fastprogress.py:41\u001b[0m, in \u001b[0;36mProgressBar.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen):\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m, in \u001b[0;36mAudioCollator.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     44\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m     46\u001b[0m   \u001b[38;5;66;03m# get audio_data in tensor format\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m   audio_data \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmono\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m   \u001b[38;5;66;03m# get the label and its audio\u001b[39;00m\n\u001b[1;32m     49\u001b[0m   label, audio_data \u001b[38;5;241m=\u001b[39m compute_labels(sample, audio_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# Otherwise try soundfile first, and then fall back if necessary\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m         y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/librosa/core/audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    206\u001b[0m     context \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n\u001b[1;32m    212\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m sf_desc\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/soundfile.py:1200\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1198\u001b[0m         _os\u001b[38;5;241m.\u001b[39mclose(_os\u001b[38;5;241m.\u001b[39mopen(file, _os\u001b[38;5;241m.\u001b[39mO_WRONLY \u001b[38;5;241m|\u001b[39m _os\u001b[38;5;241m.\u001b[39mO_TRUNC))\n\u001b[1;32m   1199\u001b[0m openfunction \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_open\n\u001b[0;32m-> 1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_unicode\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1202\u001b[0m         openfunction \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_wchar_open\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epochs = 20\n",
    "\n",
    "# config for progress bar\n",
    "mb = master_bar(range(epochs))\n",
    "mb.names = ['Training loss', 'Validation loss']\n",
    "x = []\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "valid_mean_min = np.Inf\n",
    "\n",
    "for epoch in tqdm(mb):\n",
    "  x.append(epoch)\n",
    "  # Evaluate\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  #pbar = tqdm(train_dl, total=len(train_dl), position=0, desc=\"Training\", leave=True)\n",
    "  for batch in progress_bar(train_dl, parent=mb):\n",
    "    audio_data = batch['audio'].to(device)\n",
    "    \n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    # get mel spectograms\n",
    "    mel_audio_data = audio_transform(audio_data)\n",
    "    # do zmuv transform\n",
    "    mel_audio_data = zmuv_transform(mel_audio_data)\n",
    "    predicted_scores = model(mel_audio_data.unsqueeze(1))\n",
    "#     get loss\n",
    "#     predicted_scores = torch.argmax(predicted_scores, dim=1)\n",
    "    loss = criterion(predicted_scores, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "#     model.zero_grad()\n",
    "\n",
    "    # backward propagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "      \n",
    "  for group in optimizer.param_groups:\n",
    "    group[\"lr\"] *= lr_decay\n",
    "\n",
    "  mean = total_loss / len(train_dl)\n",
    "  training_losses.append(mean.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d17c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
