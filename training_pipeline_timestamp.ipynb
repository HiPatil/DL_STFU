{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd423b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchaudio.transforms import MelSpectrogram, ComputeDeltas\n",
    "\n",
    "from torch.optim.adamw import AdamW\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import tqdm\n",
    "import IPython\n",
    "import re\n",
    "\n",
    "import whisper_timestamped as whisper\n",
    "import librosa\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import ast\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c093b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wake_words = [\n",
    "    \"fuck\"\n",
    "    ,\"fucking\"\n",
    "    ,\"ass\"\n",
    "    ,\"bitch\"\n",
    "    ,\"dick\"\n",
    "    ,\"fag\"\n",
    "    ,\"faggot\"\n",
    "    ,\"shit\"\n",
    "    ,\"cunt\"\n",
    "    ,\"whore\"\n",
    "    ,\"cock\"\n",
    "    ,\"pussy\"\n",
    "    ,\"cocksucker\"\n",
    "\n",
    "]\n",
    "\n",
    "wake_words_sequence = [\"0\", \"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f85c694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wake_word_seq_map = dict(zip(wake_words, wake_words_sequence))\n",
    "regex_pattern = r\"\\b(?:{})\\b\".format(\"|\".join(map(re.escape, wake_words)))\n",
    "pattern = re.compile(regex_pattern, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def wake_words_search(pattern, word):\n",
    "    try:\n",
    "        return bool(pattern.search(word))\n",
    "    except TypeError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5827fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_data = pd.read_csv('dataset/cuss_word_time_stamp_wisper_true.csv')\n",
    "positive_train_data['timestamps'] = positive_train_data['timestamps'].apply(ast.literal_eval)\n",
    "positive_train_data['timestamps'] = positive_train_data['timestamps'].apply(lambda x: {key.replace(',', '').replace('.', ''): value for key, value in x.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9391ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dataset/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109fd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_pattern = re.compile(r\"'(?P<key>[^']+)'\")\n",
    "\n",
    "def compute_labels(metadata, audio_data):\n",
    "  label = len(wake_words) # by default negative label\n",
    "\n",
    "  # if it is generated data then \n",
    "  if metadata['sentence'].lower() in wake_words:\n",
    "    label = int(wake_word_seq_map[metadata['sentence'].lower()])\n",
    "  else:\n",
    "    # if the sentence has one wakeword get label and end timestamp\n",
    "    for word in metadata['sentence'].lower().split():\n",
    "      wake_word_found = False\n",
    "      word = re.sub('\\W+', '', word)\n",
    "      if word in wake_words:\n",
    "        wake_word_found = True\n",
    "        break\n",
    "  \n",
    "    if wake_word_found:\n",
    "      label = int(wake_word_seq_map[word])\n",
    "      if word in  metadata['timestamps']:\n",
    "        timestamps = metadata['timestamps']\n",
    "        if type(timestamps) == str:\n",
    "          timestamps = json.loads(key_pattern.sub(r'\"\\g\"', timestamps))\n",
    "        word_ts = timestamps[word]\n",
    "        audio_start_idx = int((word_ts['start'] * 1000) * sr / 1000)\n",
    "        audio_end_idx = int((word_ts['end'] * 1000) * sr / 1000)\n",
    "        audio_data = audio_data[audio_start_idx:audio_end_idx]\n",
    "      else: # if there are issues with word alignment, we might not get ts\n",
    "        label = len(wake_words)  # mark them for negative\n",
    "\n",
    "  return label, audio_data\n",
    "\n",
    "\n",
    "\n",
    "class AudioCollator(object):\n",
    "  def __init__(self, noise_set=None):\n",
    "    self.noise_set = noise_set\n",
    "\n",
    "  def __call__(self, batch):\n",
    "    batch_tensor = {}\n",
    "    window_size_ms = 750\n",
    "    max_length = int(window_size_ms/1000 * sr)\n",
    "    audio_tensors = []\n",
    "    labels = []\n",
    "    for sample in batch:\n",
    "      # get audio_data in tensor format\n",
    "      audio_data = librosa.core.load(data_path+sample['path'], sr=sr, mono=True)[0]\n",
    "      # get the label and its audio\n",
    "      label, audio_data = compute_labels(sample, audio_data)\n",
    "      audio_data_length = audio_data.size / sr * 1000 #ms\n",
    "  \n",
    "      # below is to make sure that we always got length of 12000 \n",
    "      # i.e 750 ms with sr 16000\n",
    "      # trim to max_length\n",
    "      if audio_data_length > window_size_ms:\n",
    "        # randomly trim either at start and end\n",
    "        if random.random() < 0.5:\n",
    "          audio_data = audio_data[:max_length]\n",
    "        else:\n",
    "          audio_data = audio_data[audio_data.size-max_length:]\n",
    "  \n",
    "      # pad with zeros\n",
    "      if audio_data_length < window_size_ms:\n",
    "        # randomly either append or prepend\n",
    "        if random.random() < 0.5:\n",
    "          audio_data = np.append(audio_data, np.zeros(int(max_length - audio_data.size)))\n",
    "        else:\n",
    "          audio_data = np.append(np.zeros(int(max_length - audio_data.size)), audio_data)\n",
    "\n",
    "      # Add noise\n",
    "      if self.noise_set:\n",
    "        noise_level =  random.randint(1, 5)/10 # 10 to 50%\n",
    "        noise_sample = librosa.core.load(self.noise_set[random.randint(0,len(self.noise_set)-1)], sr=sr, mono=True)[0]\n",
    "        # randomly select first or last seq of noise\n",
    "        if random.random() < 0.5:\n",
    "          audio_data = (1 - noise_level) * audio_data +  noise_level * noise_sample[:max_length]\n",
    "        else:\n",
    "          audio_data = (1 - noise_level) * audio_data +  noise_level * noise_sample[-max_length:]\n",
    "  \n",
    "      audio_tensors.append(torch.from_numpy(audio_data))\n",
    "      labels.append(label)\n",
    "  \n",
    "    batch_tensor = {\n",
    "        'audio': torch.stack(audio_tensors),\n",
    "        'labels': torch.tensor(labels) \n",
    "    }\n",
    "    \n",
    "    return batch_tensor\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efc9f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = positive_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adc9d4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fuck word 19.54486416278818 %\n",
      "Total ass word 3.144279683948479 %\n",
      "Total fucking word 42.78872172313021 %\n"
     ]
    }
   ],
   "source": [
    "hey_pattern = re.compile(r'\\bfuck\\b', flags=re.IGNORECASE)\n",
    "fourth_pattern = re.compile(r'\\bass\\b', flags=re.IGNORECASE)\n",
    "brain_pattern = re.compile(r'\\bfucking\\b', flags=re.IGNORECASE)\n",
    "\n",
    "print(f\"Total fuck word {(train_ds[[wake_words_search(hey_pattern, sentence) for sentence in train_ds['sentence']]].size/train_ds.size) * 100} %\")\n",
    "print(f\"Total ass word {(train_ds[[wake_words_search(fourth_pattern, sentence) for sentence in train_ds['sentence']]].size/train_ds.size) * 100} %\")\n",
    "print(f\"Total fucking word {(train_ds[[wake_words_search(brain_pattern, sentence) for sentence in train_ds['sentence']]].size/train_ds.size) * 100} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a985ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_test = glob.glob('dataset/MS-SNSD/noise_test/*.wav')\n",
    "noise_train = glob.glob('dataset/MS-SNSD/noise_train/*.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9805489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 0\n",
    "\n",
    "train_audio_collator = AudioCollator(noise_set=noise_train)\n",
    "train_dl = tud.DataLoader(train_ds.to_dict(orient='records'),\n",
    "                  batch_size=batch_size,\n",
    "                  drop_last=True,\n",
    "                  shuffle=True,\n",
    "                  num_workers=num_workers,\n",
    "                  collate_fn=train_audio_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a433614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 24000\n",
    "\n",
    "num_mels = 40 # https://en.wikipedia.org/wiki/Mel_scale\n",
    "num_fft = 512 # window length - Fast Fourier Transform\n",
    "hop_length = 200  # making hops of size hop_length each time to sample the next window\n",
    "\n",
    "def audio_transform(audio_data):\n",
    "  # Transformations\n",
    "  # Mel-scale spectrogram is a combination of Spectrogram and mel scale conversion\n",
    "  # 1. compute FFT - for each window to transform from time domain to frequency domain\n",
    "  # 2. Generate Mel Scale - Take entire freq spectrum & seperate to n_mels evenly spaced\n",
    "  #    frequencies. (not by distance on freq domain but distance as it is heard by human ear)\n",
    "  # 3. Generate Spectrogram - For each window, decompose the magnitude of the signal\n",
    "  #    into its components, corresponding to the frequencies in the mel scale. \n",
    "  mel_spectrogram  = MelSpectrogram(n_mels=num_mels,\n",
    "                                    sample_rate=sr,\n",
    "                                    n_fft=num_fft,\n",
    "                                    hop_length=hop_length,\n",
    "                                    norm='slaney')\n",
    "  mel_spectrogram.to(device)\n",
    "  log_mels = mel_spectrogram(audio_data.float()).add_(1e-7).log_().contiguous()\n",
    "  # returns (channel, n_mels, time) \n",
    "  return log_mels.to(device)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f53c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "class ZmuvTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer('total', torch.zeros(1))\n",
    "        self.register_buffer('mean', torch.zeros(1))\n",
    "        self.register_buffer('mean2', torch.zeros(1))\n",
    "\n",
    "    def update(self, data, mask=None):\n",
    "        with torch.no_grad():\n",
    "            if mask is not None:\n",
    "                data = data * mask\n",
    "                mask_size = mask.sum().item()\n",
    "            else:\n",
    "                mask_size = data.numel()\n",
    "            self.mean = (data.sum() + self.mean * self.total) / (self.total + mask_size)\n",
    "            self.mean2 = ((data ** 2).sum() + self.mean2 * self.total) / (self.total + mask_size)\n",
    "            self.total += mask_size\n",
    "\n",
    "    def initialize(self, iterable: Iterable[torch.Tensor]):\n",
    "        for ex in iterable:\n",
    "            self.update(ex)\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return (self.mean2 - self.mean ** 2).sqrt()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / self.std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06866db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"#torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19fc3b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean is 0.000012\n",
      "Standard Deviation is 0.104256\n"
     ]
    }
   ],
   "source": [
    "zmuv_audio_collator = AudioCollator()\n",
    "zmuv_dl = tud.DataLoader(train_ds.to_dict(orient='records'),\n",
    "                  batch_size=1,\n",
    "                  num_workers=num_workers,\n",
    "                  collate_fn=zmuv_audio_collator)\n",
    "\n",
    "zmuv_transform = ZmuvTransform().to(device)\n",
    "if Path(\"zmuv.pt.bin\").exists():\n",
    "    zmuv_transform.load_state_dict(torch.load(str(\"zmuv.pt.bin\")))\n",
    "else:\n",
    "  for idx, batch in enumerate(zmuv_dl):\n",
    "    zmuv_transform.update(batch['audio'].to(device))\n",
    "  print(dict(zmuv_mean=zmuv_transform.mean, zmuv_std=zmuv_transform.std))\n",
    "  torch.save(zmuv_transform.state_dict(), str(\"zmuv.pt.bin\"))\n",
    "\n",
    "print(f\"Mean is {zmuv_transform.mean.item():0.6f}\")\n",
    "print(f\"Standard Deviation is {zmuv_transform.std.item():0.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fef5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_labels, num_maps1, num_maps2, num_hidden_input, hidden_size):\n",
    "        super(CNN, self).__init__()\n",
    "        conv0 = nn.Conv2d(1, num_maps1, (8, 16), padding=(4, 0), stride=(2, 2), bias=True)\n",
    "        pool = nn.MaxPool2d(2)\n",
    "        conv1 = nn.Conv2d(num_maps1, num_maps2, (5, 5), padding=2, stride=(2, 1), bias=True)\n",
    "        self.num_hidden_input = num_hidden_input\n",
    "        self.encoder1 = nn.Sequential(conv0,\n",
    "                                      nn.ReLU(),\n",
    "                                      pool,\n",
    "                                      nn.BatchNorm2d(num_maps1, affine=True))\n",
    "        self.encoder2 = nn.Sequential(conv1,\n",
    "                                      nn.ReLU(),\n",
    "                                      pool,\n",
    "                                      nn.BatchNorm2d(num_maps2, affine=True))\n",
    "        self.output = nn.Sequential(nn.Linear(num_hidden_input, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(0.1),\n",
    "                                    nn.Linear(hidden_size, num_labels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 1, 3, 2)  # change to (time, n_mels)\n",
    "        # pass through first conv layer\n",
    "        x1 = self.encoder1(x)\n",
    "        # pass through second conv layer\n",
    "        x2 = self.encoder2(x1)\n",
    "        # flattening - keep first dim batch same, flatten last 3 dims\n",
    "        x = x2.view(x2.size(0), -1)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8b308c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(wake_words) # oov\n",
    "num_maps1  = 48\n",
    "num_maps2  = 64\n",
    "num_hidden_input =  1152\n",
    "hidden_size = 128\n",
    "model = CNN(num_labels, num_maps1, num_maps2, num_hidden_input, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8909dbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (encoder1): Sequential(\n",
       "    (0): Conv2d(1, 48, kernel_size=(8, 16), stride=(2, 2), padding=(4, 0))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (encoder2): Sequential(\n",
       "    (0): Conv2d(48, 64, kernel_size=(5, 5), stride=(2, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (output): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=13, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78ac017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 48, 46, 13]           6,192\n",
      "              ReLU-2           [-1, 48, 46, 13]               0\n",
      "         MaxPool2d-3            [-1, 48, 23, 6]               0\n",
      "         MaxPool2d-4            [-1, 48, 23, 6]               0\n",
      "       BatchNorm2d-5            [-1, 48, 23, 6]              96\n",
      "            Conv2d-6            [-1, 64, 12, 6]          76,864\n",
      "              ReLU-7            [-1, 64, 12, 6]               0\n",
      "         MaxPool2d-8             [-1, 64, 6, 3]               0\n",
      "         MaxPool2d-9             [-1, 64, 6, 3]               0\n",
      "      BatchNorm2d-10             [-1, 64, 6, 3]             128\n",
      "           Linear-11                  [-1, 128]         147,584\n",
      "             ReLU-12                  [-1, 128]               0\n",
      "          Dropout-13                  [-1, 128]               0\n",
      "           Linear-14                   [-1, 13]           1,677\n",
      "================================================================\n",
      "Total params: 232,541\n",
      "Trainable params: 232,541\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.69\n",
      "Params size (MB): 0.89\n",
      "Estimated Total Size (MB): 1.59\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.to('cuda'), input_size=(1,40,91))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a2b065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001 # Weight regularization\n",
    "lr_decay = 0.95\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(filter(lambda x: x.requires_grad, model.parameters()))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a952c115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/20 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='2309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/2309 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# backward propagation\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\carla\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\carla\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "# config for progress bar\n",
    "mb = master_bar(range(epochs))\n",
    "mb.names = ['Training loss', 'Validation loss']\n",
    "x = []\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "valid_mean_min = np.Inf\n",
    "\n",
    "for epoch in mb:\n",
    "  x.append(epoch)\n",
    "  # Evaluate\n",
    "  model.train()\n",
    "  total_loss = torch.Tensor([0.0]).to(device)\n",
    "  #pbar = tqdm(train_dl, total=len(train_dl), position=0, desc=\"Training\", leave=True)\n",
    "  for batch in progress_bar(train_dl, parent=mb):\n",
    "    audio_data = batch['audio'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    # get mel spectograms\n",
    "    mel_audio_data = audio_transform(audio_data)\n",
    "    # do zmuv transform\n",
    "    mel_audio_data = zmuv_transform(mel_audio_data)\n",
    "    predicted_scores = model(mel_audio_data.unsqueeze(1))\n",
    "    # get loss\n",
    "    predicted_scores = torch.max(predicted_scores, 1)[1].float()\n",
    "    loss = criterion(predicted_scores, labels.float())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # backward propagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss += loss\n",
    "      \n",
    "  for group in optimizer.param_groups:\n",
    "    group[\"lr\"] *= lr_decay\n",
    "\n",
    "  mean = total_loss / len(train_dl)\n",
    "  training_losses.append(mean.cpu())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
